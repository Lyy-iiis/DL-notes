# Guidance in Diffusion Models

### Classifier-Free Guidance

What is the problem of classifier guidance? $p_\phi(y|x_t)$ is not accurate since $x_t$ is generated by diffusion process.

Model $\nabla_{x_t}\log p_t(y|x_t)$ using Bayesian

$$
\nabla_{x_t}\log p_t(y|x_t)=\nabla_{x_t}\log p_t(x_t|y)-\nabla_{x_t}\log p_t(x_t)\\
=-\frac{1}{\sqrt{1-\overline{\alpha}_t}}\left(\epsilon(x_t,t,y)-\epsilon(x_t,t)\right)
$$

Where the proof of second equation is [here](https://en.wikipedia.org/wiki/Diffusion_model#:~:text=DDPM%20and%20score%2Dbased%20generative,a%20NCSN%2C%20and%20vice%20versa.&text=Thus%2C%20a%20score%2Dbased%20network,be%20used%20for%20denoising%20diffusion.&text=Thus%2C%20a%20denoising%20network%20can,as%20for%20score%2Dbased%20diffusion.)

Hence, we can generate using

$$
\hat{\epsilon}=\epsilon(x_t,t,y)-\sqrt{1-\overline{\alpha}_t}w\nabla_{x_t}\log p_\phi(y|x_t)\\
=(1+w)\epsilon(x_t,t,y)-w\epsilon(x_t,t)
$$

where $w$ is a hyperparameter.

### Model Guidance

To enhance both generation quality and alignment to conditions, we propose to take into account the posterior probability $p_\theta(y|x_t)$, i.e. joint optimizing

$$
\tilde{p}_\theta(x_t|y) = p_\theta(x_t|y) p_\theta(y|x_t)^w
$$

employ Bayesian rule to get
$$
% \nabla \log \tilde{p}_\theta(x_t|y) =\nabla  \log p_\theta(x_t|y) + w\nabla \log p_\theta(y|x_t)
\nabla_{x_t}\log p_\theta(y|x_t) = \nabla_{x_t}\log p_\theta(x_t|y) - \nabla_{x_t}\log p_\theta(x_t)
$$

and we use the diffusion model to approximate the scores
$$
\nabla_{x_t}\log p_\theta(x_t) = -\frac{1}{\sigma_t} \epsilon_\theta(x_t,t,\emptyset)\\
\nabla_{x_t}\log p_\theta(x_t|y) = -\frac{1}{\sigma_t} \epsilon_\theta(x_t,t,y)
$$

instead of train to predict $\epsilon$ ($\nabla_{x_t}\log p_\theta(x_t|y)$), we train to predict $\nabla_{x_t}\log \tilde{p}_\theta(x_t|y)$ jointly, i.e. our target becomes 
$$
\epsilon+w(\epsilon_\theta(x_t,t,c) - \epsilon_\theta(x_t,t,\emptyset))
$$
To stabilize the training, we use EMA model $\tilde{\epsilon}_\theta$ as target and do stop gradient
$$
\mathcal{L}_{mg} = \mathbb{E}_{t,(x_0,c),\epsilon} \left[ \left\| \epsilon(x_t,t,c)-\epsilon' \right\|^2 \right]\\
\epsilon' = \epsilon + w\text{sg}(\tilde{\epsilon}_\theta(x_t,t,c) - \tilde{\epsilon}_\theta(x_t,t,\emptyset))
$$

can also apply to flow matching

### Guidance-Free Guidance

Our model $\epsilon_\theta^c(x_t|c)$ and target sampling model $\epsilon_\theta^s(x_t|c)$ follows

$$
\epsilon_\theta^c(x_t|c) = \frac{1}{1+w}\epsilon^s_\theta(x_t|c) + \frac{w}{1+w}\epsilon_\theta^u(x_t)
$$

Training objective is
$$
\min_\theta \mathbb{E}_{p(x,c),t,\epsilon} \left[ \left\| \frac{1}{1+w}\epsilon_\theta^s(x_t|c) + \frac{w}{1+w}\epsilon_\theta^u(x_t) - \epsilon\right\|^2 \right]
$$

Implementation:
1. reparameterize $\beta=\frac{1}{1+w}$
2. apply stop gradient on unconditioned model
3. train with different $\beta$, add $\beta$ to conditioning of $\epsilon_\theta^s(x_t|c)$
4. random dropout $c$ with probability $p$ during training
$$
\mathcal{L}_{gf} = \mathbb{E}_{p(x,c),t,\epsilon,\beta} \left[ \left\| \beta\epsilon_\theta^s(x_t|c_\empty,\beta) + (1-\beta)\text{sg}(\epsilon_\theta^u(x_t|\emptyset, 1)) - \epsilon\right\|^2 \right]
$$

**Can both train from scratch and finetune.**